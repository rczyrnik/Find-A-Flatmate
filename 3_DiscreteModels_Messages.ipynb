{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCLUDES:\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# DECISION TREE CLASSIFIER\n",
    "\n",
    "# RANDOM FOREST CLASSIFIER\n",
    "\n",
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import my_pickle as mp\n",
    "import my_resample as ms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from scipy import interp\n",
    "# from random import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "# from sklearn.ensemble.partial_dependence import partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mp.unjson_it('data_X')\n",
    "y = mp.unjson_it('data_y')['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_pipeline(model, vebose=False):\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(), random_state=17)\n",
    "\n",
    "    # resample\n",
    "    X_train, y_train = ms.oversample(X_train, y_train, .5)\n",
    "\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # fit data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return recall_score(y_test, y_pred), precision_score(y_test, y_pred), model.score(X_test, y_test)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nMETRICS\")\n",
    "        print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "        print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "        print(\"Model accuracy: {}\".format(model.score(X_test, y_test)))\n",
    "\n",
    "        print (\"\\nCONFUSION MATRIX\")\n",
    "        print (confusion_matrix(y_test, y_pred))\n",
    "        print (\"\\nkey:\")\n",
    "        print (\" TN   FP \")\n",
    "        print (\" FN   TP \")\n",
    "\n",
    "        pred_all_0 = [0]*len(y_test)\n",
    "        pred_all_1 = [1]*len(y_test)\n",
    "        pred_50_50 = np.random.choice([0,1], size=len(y_test))\n",
    "        pred_90_10 = np.random.choice([0,1], size=len(y_test), p=[.9,.1])\n",
    "        \n",
    "        print(\"\\nRECALL AND ACCURACY FOR DIFFERNET MODELS\")\n",
    "        print(\"recall     \\t precision   \\tmodel\")\n",
    "        print(recall_score(y_test, y_pred), '\\t',precision_score(y_test, y_pred), \"my model\")\n",
    "        print(recall_score(y_test, pred_all_0),'\\t','\\t', precision_score(y_test, pred_all_0), \"\\t\\tpredict all zero\")\n",
    "        print(recall_score(y_test, pred_all_1),'\\t','\\t', precision_score(y_test, pred_all_1), \"predict all one\")\n",
    "        print(recall_score(y_test, pred_50_50),'\\t', precision_score(y_test, pred_50_50), \"predict 50-50\")\n",
    "        print(recall_score(y_test, pred_90_10), precision_score(y_test, pred_90_10), \"predict 90-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS\n",
      "Model recall: 0.16040955631399317\n",
      "Model precision: 0.42342342342342343\n",
      "Model accuracy: 0.888569374550683\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[2425   64]\n",
      " [ 246   47]]\n",
      "\n",
      "key:\n",
      " TN   FP \n",
      " FN   TP \n",
      "\n",
      "RECALL AND ACCURACY FOR DIFFERNET MODELS\n",
      "recall     \t precision   \tmodel\n",
      "0.160409556314 \t 0.423423423423 my model\n",
      "0.0 \t \t 0.0 \t\tpredict all zero\n",
      "1.0 \t \t 0.105319913731 predict all one\n",
      "0.518771331058 \t 0.106741573034 predict 50-50\n",
      "0.116040955631 0.114093959732 predict 90-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gandalf/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t len_about_receiver \t 0.0957033179665\n",
      "2 \t distance \t 0.0889721461666\n",
      "3 \t age_receiver \t 0.0580360146242\n",
      "4 \t maxCost_receiver \t 0.0526454413219\n",
      "5 \t urgency_receiver \t 0.0391853989622\n",
      "6 \t urgency_sender \t 0.0382053211554\n",
      "7 \t age_dif \t 0.0377325737466\n",
      "8 \t maxCost_sender \t 0.0357812383137\n",
      "9 \t len_about_sender \t 0.0331946526687\n",
      "10 \t minCost_receiver \t 0.0319950757122\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS\n",
      "Model recall: 0.5563139931740614\n",
      "Model precision: 0.21419185282522996\n",
      "Model accuracy: 0.7383177570093458\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[1891  598]\n",
      " [ 130  163]]\n",
      "\n",
      "key:\n",
      " TN   FP \n",
      " FN   TP \n",
      "\n",
      "RECALL AND ACCURACY FOR DIFFERNET MODELS\n",
      "recall     \t precision   \tmodel\n",
      "0.556313993174 \t 0.214191852825 my model\n",
      "0.0 \t \t 0.0 \t\tpredict all zero\n",
      "1.0 \t \t 0.105319913731 predict all one\n",
      "0.481228668942 \t 0.102396514161 predict 50-50\n",
      "0.0648464163823 0.0703703703704 predict 90-10\n",
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t urgency_receiver \t 0.102568827974\n",
      "2 \t urgency_sender \t 0.080896654926\n",
      "3 \t len_about_receiver \t 0.0789943268579\n",
      "4 \t age_receiver \t 0.0747287291074\n",
      "5 \t distance \t 0.0605902757686\n",
      "6 \t isStudent_receiver \t 0.0534049651658\n",
      "7 \t maxCost_receiver \t 0.0447464616228\n",
      "8 \t age_dif \t 0.0392171997138\n",
      "9 \t numRoommates_receiver \t 0.0328159680465\n",
      "10 \t period_count_receiver \t 0.029789745294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gandalf/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS\n",
      "Model recall: 0.20477815699658702\n",
      "Model precision: 0.21052631578947367\n",
      "Model accuracy: 0.8353702372393961\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[2264  225]\n",
      " [ 233   60]]\n",
      "\n",
      "key:\n",
      " TN   FP \n",
      " FN   TP \n",
      "\n",
      "RECALL AND ACCURACY FOR DIFFERNET MODELS\n",
      "recall     \t precision   \tmodel\n",
      "0.204778156997 \t 0.210526315789 my model\n",
      "0.0 \t \t 0.0 \t\tpredict all zero\n",
      "1.0 \t \t 0.105319913731 predict all one\n",
      "0.535836177474 \t 0.115952732644 predict 50-50\n",
      "0.0921501706485 0.100746268657 predict 90-10\n",
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t len_about_receiver \t 0.0984110870658\n",
      "2 \t distance \t 0.0894790715008\n",
      "3 \t age_receiver \t 0.0558216963682\n",
      "4 \t urgency_receiver \t 0.0494739584059\n",
      "5 \t minCost_receiver \t 0.0445277172083\n",
      "6 \t urgency_sender \t 0.0405899938056\n",
      "7 \t len_about_sender \t 0.0383268092995\n",
      "8 \t maxCost_sender \t 0.0377616510852\n",
      "9 \t age_sender \t 0.036495401473\n",
      "10 \t age_dif \t 0.0352120226206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gandalf/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "METRICS\n",
      "Model recall: 0.07167235494880546\n",
      "Model precision: 0.1242603550295858\n",
      "Model accuracy: 0.8490294751976994\n",
      "\n",
      "CONFUSION MATRIX\n",
      "[[2341  148]\n",
      " [ 272   21]]\n",
      "\n",
      "key:\n",
      " TN   FP \n",
      " FN   TP \n",
      "\n",
      "RECALL AND ACCURACY FOR DIFFERNET MODELS\n",
      "recall     \t precision   \tmodel\n",
      "0.0716723549488 \t 0.12426035503 my model\n",
      "0.0 \t \t 0.0 \t\tpredict all zero\n",
      "1.0 \t \t 0.105319913731 predict all one\n",
      "0.477815699659 \t 0.103934669636 predict 50-50\n",
      "0.0887372013652 0.0884353741497 predict 90-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gandalf/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "do_stuff(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([X.columns,model.coef_[0]]).T\n",
    "results_df.columns = ['feature','coefficients']\n",
    "results_df['abs_val'] = results_df.coefficients.apply(abs)\n",
    "results_df['sign'] = results_df.coefficients.apply(lambda s: s>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficients</th>\n",
       "      <th>abs_val</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age_receiver</td>\n",
       "      <td>2.42751e-20</td>\n",
       "      <td>2.427509e-20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I_count_receiver</td>\n",
       "      <td>1.359e-20</td>\n",
       "      <td>1.358998e-20</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I_count_sender</td>\n",
       "      <td>6.26409e-21</td>\n",
       "      <td>6.264091e-21</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age_sender</td>\n",
       "      <td>-4.63331e-21</td>\n",
       "      <td>4.633312e-21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age_dif</td>\n",
       "      <td>-1.34447e-21</td>\n",
       "      <td>1.344466e-21</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature coefficients       abs_val   sign\n",
       "3      age_receiver  2.42751e-20  2.427509e-20   True\n",
       "0  I_count_receiver    1.359e-20  1.358998e-20   True\n",
       "1    I_count_sender  6.26409e-21  6.264091e-21   True\n",
       "4        age_sender -4.63331e-21  4.633312e-21  False\n",
       "2           age_dif -1.34447e-21  1.344466e-21  False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head().sort_values('abs_val', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.set_index('feature').loc['age_receiver'].sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_grid_search(X, y):\n",
    "    '''\n",
    "    X as 2d numpy array\n",
    "    y as 1d numpy array\n",
    "    \n",
    "    PARAMETERS\n",
    "    n_estimators: The number of trees in the forest\n",
    "    criterion: gini or entropy\n",
    "    max_features: The number of features to consider when looking for the best split\n",
    "        If int, then consider max_features features at each split.\n",
    "        If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "        If “auto”, then max_features=sqrt(n_features).\n",
    "        If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        If “log2”, then max_features=log2(n_features).\n",
    "        If None, then max_features=n_features.\n",
    "    max_depth: The maximum depth of the tree\n",
    "    n_jobs: The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n",
    "    '''\n",
    "    \n",
    "    # Split it up into our training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # resample\n",
    "    X_train, y_train = ms.oversample(X_train.as_matrix(), y_train.as_matrix(), .5)\n",
    "    \n",
    "    # Initalize our model here\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Here are the params we are tuning\n",
    "    param_grid = {'max_features' : [None],\n",
    "                  'n_estimators' : [50,100,1000],\n",
    "                  'max_depth': [50],\n",
    "                  'min_samples_leaf': [3]\n",
    "                  }\n",
    "\n",
    "    # Plug in our model, params dict, and the number of jobs, then .fit()\n",
    "    gs_cv = GridSearchCV(model, param_grid, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    # return the best score and the best params\n",
    "    return gs_cv.best_score_, gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n = do_grid_search(X, y)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression()\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for feature importances across all trees\n",
    "\n",
    "n = len(X.columns)\n",
    "\n",
    "#importances = forest_fit.feature_importances_[:n]\n",
    "importances = model.feature_importances_[:n]\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "\n",
    "# print(\"Feature ranking:\")\n",
    "# for f in range(n):\n",
    "#     print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(n), importances[indices], yerr=std[indices], color=\"r\", align=\"edge\", width = -.9)\n",
    "plt.xticks(range(n), features, rotation=-75)\n",
    "plt.xlim([-1, n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try modifying the number of trees, graph results\n",
    "num_trees = range(5, 50, 5)\n",
    "accuracies = []\n",
    "for n in num_trees:\n",
    "    tot = 0\n",
    "    for i in range(5):\n",
    "        rf = RandomForestClassifier(n_estimators=n)\n",
    "        rf.fit(X_train, y_train)\n",
    "        tot += rf.score(X_test, y_test)\n",
    "    accuracies.append(tot / 5)\n",
    "plt.plot(num_trees, accuracies)\n",
    "plt.xlabel=\"num_trees\"\n",
    "plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modifying the max features parameter\n",
    "for nn in range(10):\n",
    "    num_features = range(2, len(X.columns))\n",
    "    accuracies = []\n",
    "    for n in num_features:\n",
    "        tot = 0\n",
    "        for i in range(5):\n",
    "            rf = RandomForestClassifier(max_features=n)\n",
    "            rf.fit(X_train, y_train)\n",
    "            tot += rf.score(X_test, y_test)\n",
    "        accuracies.append(tot / 5)\n",
    "    plt.plot(num_features, accuracies)\n",
    "    plt.xlabel=\"num_features\"\n",
    "    plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run all the other classifiers that we have learned so far in class\n",
    "def get_scores(classifier, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    model = classifier(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    return model.score(X_test, y_test), \\\n",
    "           precision_score(y_test, y_predict), \\\n",
    "           recall_score(y_test, y_predict)\n",
    "\n",
    "print (\"    Model,                Accuracy, Precision, Recall\")\n",
    "print (\"    Random Forest:       \", get_scores(RandomForestClassifier, X_train, X_test, y_train, y_test, n_estimators=25, max_features=5))\n",
    "print (\"    Logistic Regression: \", get_scores(LogisticRegression, X_train, X_test, y_train, y_test))\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(X, y, clf_class, title, **kwargs):\n",
    "# def plot_roc(X, y, clf_class, kwargs):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= len(kf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "    plt.title(title + 'ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xfake, yfake = np.random.rand(100,6), np.random.randint(0,2, size=(100,))\n",
    "plt.figure(figsize=(20,16))\n",
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(xfake, yfake, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "#plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "#plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(X, y, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
