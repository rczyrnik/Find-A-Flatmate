{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCLUDES:\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# DECISION TREE CLASSIFIER\n",
    "\n",
    "# RANDOM FOREST CLASSIFIER\n",
    "\n",
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import my_pickle as mp\n",
    "import my_resample as ms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from scipy import interp\n",
    "# from random import *\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "# from sklearn.ensemble.partial_dependence import partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = mp.unjson_it('data_X')\n",
    "y = mp.unjson_it('data_y')['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_pipeline(model, vebose=False):\n",
    "    \n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.as_matrix(), y.as_matrix(), random_state=17)\n",
    "\n",
    "    # resample\n",
    "    X_train, y_train = ms.oversample(X_train, y_train, .5)\n",
    "\n",
    "    # scale data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # fit data\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    return recall_score(y_test, y_pred), precision_score(y_test, y_pred), model.score(X_test, y_test)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nMETRICS\")\n",
    "        print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "        print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "        print(\"Model accuracy: {}\".format(model.score(X_test, y_test)))\n",
    "\n",
    "        print (\"\\nCONFUSION MATRIX\")\n",
    "        print (confusion_matrix(y_test, y_pred))\n",
    "        print (\"\\nkey:\")\n",
    "        print (\" TN   FP \")\n",
    "        print (\" FN   TP \")\n",
    "\n",
    "        pred_all_0 = [0]*len(y_test)\n",
    "        pred_all_1 = [1]*len(y_test)\n",
    "        pred_50_50 = np.random.choice([0,1], size=len(y_test))\n",
    "        pred_90_10 = np.random.choice([0,1], size=len(y_test), p=[.9,.1])\n",
    "        \n",
    "        print(\"\\nRECALL AND ACCURACY FOR DIFFERNET MODELS\")\n",
    "        print(\"recall     \\t precision   \\tmodel\")\n",
    "        print(recall_score(y_test, y_pred), '\\t',precision_score(y_test, y_pred), \"my model\")\n",
    "        print(recall_score(y_test, pred_all_0),'\\t','\\t', precision_score(y_test, pred_all_0), \"\\t\\tpredict all zero\")\n",
    "        print(recall_score(y_test, pred_all_1),'\\t','\\t', precision_score(y_test, pred_all_1), \"predict all one\")\n",
    "        print(recall_score(y_test, pred_50_50),'\\t', precision_score(y_test, pred_50_50), \"predict 50-50\")\n",
    "        print(recall_score(y_test, pred_90_10), precision_score(y_test, pred_90_10), \"predict 90-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t len_about_receiver \t 0.0988381303038\n",
      "2 \t distance \t 0.0832252600436\n",
      "3 \t age_receiver \t 0.0544958073081\n",
      "4 \t urgency_receiver \t 0.0398327726193\n",
      "5 \t maxCost_receiver \t 0.0394533361067\n",
      "6 \t urgency_sender \t 0.0372047269166\n",
      "7 \t age_dif \t 0.0358913421377\n",
      "8 \t minCost_receiver \t 0.0343980975537\n",
      "9 \t maxCost_sender \t 0.0334170333726\n",
      "10 \t age_sender \t 0.032557205414\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "my_pipeline(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t urgency_receiver \t 0.0933045845817\n",
      "2 \t len_about_receiver \t 0.0812849415684\n",
      "3 \t urgency_sender \t 0.076735402526\n",
      "4 \t age_receiver \t 0.0632789176392\n",
      "5 \t distance \t 0.0609369343924\n",
      "6 \t maxCost_receiver \t 0.0427149855933\n",
      "7 \t isStudent_receiver \t 0.0399518297071\n",
      "8 \t minCost_receiver \t 0.039009425811\n",
      "9 \t minCost_sender \t 0.035899547427\n",
      "10 \t isStudent_sender \t 0.0357758092025\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "my_pipeline(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t urgency_receiver \t 0.0845827580673\n",
      "2 \t distance \t 0.0794751188421\n",
      "3 \t urgency_sender \t 0.0740987830925\n",
      "4 \t age_receiver \t 0.0664020017791\n",
      "5 \t len_about_receiver \t 0.0625996862229\n",
      "6 \t isStudent_receiver \t 0.0549342055358\n",
      "7 \t maxCost_receiver \t 0.042033882289\n",
      "8 \t numRoommates_receiver \t 0.0372404641354\n",
      "9 \t hobbies_overlap \t 0.0329627279865\n",
      "10 \t minCost_receiver \t 0.0268673731884\n"
     ]
    }
   ],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "my_pipeline(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE RANKINGS\n",
      "1 \t len_about_receiver \t 0.109298208343\n",
      "2 \t distance \t 0.0972166013335\n",
      "3 \t maxCost_receiver \t 0.0556132585346\n",
      "4 \t age_sender \t 0.0477451717221\n",
      "5 \t urgency_sender \t 0.0428403041314\n",
      "6 \t age_receiver \t 0.0395130755355\n",
      "7 \t urgency_receiver \t 0.0377864859871\n",
      "8 \t maxCost_sender \t 0.0359674708658\n",
      "9 \t len_about_sender \t 0.032617029945\n",
      "10 \t hobbies_overlap \t 0.0314574510996\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "my_pipeline(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.60409556313993173, 0.17134559535333979, 0.65061107117181882)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "my_pipeline(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([X.columns,model.coef_[0]]).T\n",
    "results_df.columns = ['feature','coefficients']\n",
    "results_df['abs_val'] = results_df.coefficients.apply(abs)\n",
    "results_df['sign'] = results_df.coefficients.apply(lambda s: s>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>coefficients</th>\n",
       "      <th>abs_val</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>age_receiver</td>\n",
       "      <td>0.25432</td>\n",
       "      <td>0.254320</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>age_dif</td>\n",
       "      <td>-0.182487</td>\n",
       "      <td>0.182487</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>age_sender</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>0.111513</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I_count_sender</td>\n",
       "      <td>0.0418411</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I_count_receiver</td>\n",
       "      <td>0.0243709</td>\n",
       "      <td>0.024371</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature coefficients   abs_val   sign\n",
       "3      age_receiver      0.25432  0.254320   True\n",
       "2           age_dif    -0.182487  0.182487  False\n",
       "4        age_sender     0.111513  0.111513   True\n",
       "1    I_count_sender    0.0418411  0.041841   True\n",
       "0  I_count_receiver    0.0243709  0.024371   True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.head().sort_values('abs_val', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.set_index('feature').loc['age_receiver'].sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_grid_search(X, y):\n",
    "    '''\n",
    "    X as 2d numpy array\n",
    "    y as 1d numpy array\n",
    "    \n",
    "    PARAMETERS\n",
    "    n_estimators: The number of trees in the forest\n",
    "    criterion: gini or entropy\n",
    "    max_features: The number of features to consider when looking for the best split\n",
    "        If int, then consider max_features features at each split.\n",
    "        If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "        If “auto”, then max_features=sqrt(n_features).\n",
    "        If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        If “log2”, then max_features=log2(n_features).\n",
    "        If None, then max_features=n_features.\n",
    "    max_depth: The maximum depth of the tree\n",
    "    n_jobs: The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n",
    "    '''\n",
    "    \n",
    "    # Split it up into our training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # resample\n",
    "    X_train, y_train = ms.oversample(X_train.as_matrix(), y_train.as_matrix(), .5)\n",
    "    \n",
    "    # Initalize our model here\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Here are the params we are tuning\n",
    "    param_grid = {'max_features' : [None],\n",
    "                  'n_estimators' : [50,100,1000],\n",
    "                  'max_depth': [50],\n",
    "                  'min_samples_leaf': [3]\n",
    "                  }\n",
    "\n",
    "    # Plug in our model, params dict, and the number of jobs, then .fit()\n",
    "    gs_cv = GridSearchCV(model, param_grid, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    # return the best score and the best params\n",
    "    return gs_cv.best_score_, gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n = do_grid_search(X, y)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression()\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for feature importances across all trees\n",
    "\n",
    "n = len(X.columns)\n",
    "\n",
    "#importances = forest_fit.feature_importances_[:n]\n",
    "importances = model.feature_importances_[:n]\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "\n",
    "# print(\"Feature ranking:\")\n",
    "# for f in range(n):\n",
    "#     print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(n), importances[indices], yerr=std[indices], color=\"r\", align=\"edge\", width = -.9)\n",
    "plt.xticks(range(n), features, rotation=-75)\n",
    "plt.xlim([-1, n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try modifying the number of trees, graph results\n",
    "num_trees = range(5, 50, 5)\n",
    "accuracies = []\n",
    "for n in num_trees:\n",
    "    tot = 0\n",
    "    for i in range(5):\n",
    "        rf = RandomForestClassifier(n_estimators=n)\n",
    "        rf.fit(X_train, y_train)\n",
    "        tot += rf.score(X_test, y_test)\n",
    "    accuracies.append(tot / 5)\n",
    "plt.plot(num_trees, accuracies)\n",
    "plt.xlabel=\"num_trees\"\n",
    "plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modifying the max features parameter\n",
    "for nn in range(10):\n",
    "    num_features = range(2, len(X.columns))\n",
    "    accuracies = []\n",
    "    for n in num_features:\n",
    "        tot = 0\n",
    "        for i in range(5):\n",
    "            rf = RandomForestClassifier(max_features=n)\n",
    "            rf.fit(X_train, y_train)\n",
    "            tot += rf.score(X_test, y_test)\n",
    "        accuracies.append(tot / 5)\n",
    "    plt.plot(num_features, accuracies)\n",
    "    plt.xlabel=\"num_features\"\n",
    "    plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run all the other classifiers that we have learned so far in class\n",
    "def get_scores(classifier, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    model = classifier(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    return model.score(X_test, y_test), \\\n",
    "           precision_score(y_test, y_predict), \\\n",
    "           recall_score(y_test, y_predict)\n",
    "\n",
    "print (\"    Model,                Accuracy, Precision, Recall\")\n",
    "print (\"    Random Forest:       \", get_scores(RandomForestClassifier, X_train, X_test, y_train, y_test, n_estimators=25, max_features=5))\n",
    "print (\"    Logistic Regression: \", get_scores(LogisticRegression, X_train, X_test, y_train, y_test))\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(X, y, clf_class, title, **kwargs):\n",
    "# def plot_roc(X, y, clf_class, kwargs):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= len(kf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "    plt.title(title + 'ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xfake, yfake = np.random.rand(100,6), np.random.randint(0,2, size=(100,))\n",
    "plt.figure(figsize=(20,16))\n",
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(xfake, yfake, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "#plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "#plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(X, y, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
