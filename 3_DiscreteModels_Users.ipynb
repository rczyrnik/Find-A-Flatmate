{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INCLUDES:\n",
    "\n",
    "# LOGISTIC REGRESSION\n",
    "\n",
    "# DECISION TREE CLASSIFIER\n",
    "\n",
    "# RANDOM FOREST CLASSIFIER\n",
    "\n",
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "\n",
    "import my_pickle as mp\n",
    "import my_features as mf\n",
    "import my_resample as ms\n",
    "# from my_resample import div_count_pos_neg, undersample, oversample\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = mp.unjson_it('data_user_active')\n",
    "TF = {True:1, False:0}\n",
    "X.active = X.active.map(TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = X.active\n",
    "X = X.drop(['active'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.sum()/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.amenities = X.amenities.apply(lambda x: len(x) if isinstance(x,list) else 0)\n",
    "X.hobbies = X.hobbies.apply(lambda x: len(x) if isinstance(x,list) else 0)\n",
    "X.neighborhoods = X.neighborhoods.apply(lambda x: len(x) if isinstance(x,list) else 0)\n",
    "X.location = X.location.apply(lambda x: 1 if isinstance(x,list) else 0)\n",
    "X['len_about'] = X.about.apply(len)\n",
    "X['has_about'] = X['len_about'].apply(lambda x: x>0)\n",
    "X.college = X.college.apply(lambda x: 1 if isinstance(x,str) else 0)\n",
    "X.hometownCity = X.hometownCity.apply(lambda x: 1 if isinstance(x,str) else 0)\n",
    "X.hometownCountry = X.hometownCountry.apply(lambda x: 1 if isinstance(x,str) else 0)\n",
    "X.hometownState = X.hometownState.apply(lambda x: 1 if isinstance(x,str) else 0)\n",
    "X.work = X.work.apply(lambda x: 1 if isinstance(x,str) else 0)\n",
    "\n",
    "TF = {True:1, False:0}\n",
    "TF_col = ['facebookId','has_about','has_room','linkedinId','picture','has_about']\n",
    "for col in TF_col:\n",
    "    X[col]= X[col].map(TF)\n",
    "to_drop = ['type','uid','about','metro']\n",
    "X = X.drop(to_drop, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = X.fillna(X.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X['has_about']= X['has_about'].map(TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_stuff(model):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_full.as_matrix(), y.as_matrix(), random_state=17)\n",
    "\n",
    "    X_train, y_train = ms.oversample(X_train, y_train, .5)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    print(\"\\nMETRICS\")\n",
    "    print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "    print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "    print(\"Model accuracy: {}\".format(model.score(X_test, y_test)))\n",
    "\n",
    "    pred_all_0 = [0]*len(y_test)\n",
    "    pred_all_1 = [1]*len(y_test)\n",
    "    pred_50_50 = np.random.choice([0,1], size=len(y_test))\n",
    "    pred_90_10 = np.random.choice([0,1], size=len(y_test), p=[.9,.1])\n",
    "\n",
    "    print (\"\\nCONFUSION MATRIX\")\n",
    "    print (confusion_matrix(y_test, y_pred))\n",
    "    print (\"\\nkey:\")\n",
    "    print (\" TN   FP \")\n",
    "    print (\" FN   TP \")\n",
    "\n",
    "    print(\"\\nRECALL AND ACCURACY FOR DIFFERNET MODELS\")\n",
    "    print(\"recall     \\t precision   \\tmodel\")\n",
    "    print(recall_score(y_test, y_pred), '\\t',precision_score(y_test, y_pred), \"my model\")\n",
    "    print(recall_score(y_test, pred_all_0),'\\t','\\t', precision_score(y_test, pred_all_0), \"\\t\\tpredict all zero\")\n",
    "    print(recall_score(y_test, pred_all_1),'\\t','\\t', precision_score(y_test, pred_all_1), \"predict all one\")\n",
    "    print(recall_score(y_test, pred_50_50),'\\t', precision_score(y_test, pred_50_50), \"predict 50-50\")\n",
    "    print(recall_score(y_test, pred_90_10), precision_score(y_test, pred_90_10), \"predict 90-10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 20\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1: 2pr/p+r\n",
    "check out other harmonics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADIENT BOOSTING CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 20\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "do_stuff(model)\n",
    "\n",
    "feature_importances = np.argsort(model.feature_importances_)\n",
    "top_n = 10\n",
    "print(\"\\nFEATURE RANKINGS\")\n",
    "for n in range(top_n):\n",
    "    print(n+1, '\\t',X.columns[feature_importances[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "do_stuff(model)\n",
    "\n",
    "feature_coefficients = np.argsort(model.coef_)\n",
    "top_n = 10\n",
    "# print(\"\\nFEATURE RANKINGS\")\n",
    "# for n in range(top_n):\n",
    "#     print(n+1, '\\t',X.columns[feature_coefficients[-n-1]], '\\t',sorted(model.feature_importances_)[-n-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame([X.columns,model.coef_[0]]).T\n",
    "results_df.columns = ['feature','coefficients']\n",
    "results_df['abs_val'] = results_df.coefficients.apply(abs)\n",
    "results_df['sign'] = results_df.coefficients.apply(lambda s: s>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.head().sort_values('abs_val', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_grid_search(X, y):\n",
    "    '''\n",
    "    X as 2d numpy array\n",
    "    y as 1d numpy array\n",
    "    \n",
    "    PARAMETERS\n",
    "    n_estimators: The number of trees in the forest\n",
    "    criterion: gini or entropy\n",
    "    max_features: The number of features to consider when looking for the best split\n",
    "        If int, then consider max_features features at each split.\n",
    "        If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split.\n",
    "        If “auto”, then max_features=sqrt(n_features).\n",
    "        If “sqrt”, then max_features=sqrt(n_features) (same as “auto”).\n",
    "        If “log2”, then max_features=log2(n_features).\n",
    "        If None, then max_features=n_features.\n",
    "    max_depth: The maximum depth of the tree\n",
    "    n_jobs: The number of jobs to run in parallel for both fit and predict. If -1, then the number of jobs is set to the number of cores.\n",
    "    '''\n",
    "    \n",
    "    # Split it up into our training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    # resample\n",
    "    X_train, y_train = ms.oversample(X_train.as_matrix(), y_train.as_matrix(), .5)\n",
    "    \n",
    "    # Initalize our model here\n",
    "    model = RandomForestClassifier()\n",
    "\n",
    "    # Here are the params we are tuning\n",
    "    param_grid = {'max_features' : [None],\n",
    "                  'n_estimators' : [50,100,1000],\n",
    "                  'max_depth': [50],\n",
    "                  'min_samples_leaf': [3]\n",
    "                  }\n",
    "\n",
    "    # Plug in our model, params dict, and the number of jobs, then .fit()\n",
    "    gs_cv = GridSearchCV(model, param_grid, n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "    # return the best score and the best params\n",
    "    return gs_cv.best_score_, gs_cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = RandomForestClassifier(max_depth=50, max_features=None, min_samples_leaf=3, n_estimators=100, n_jobs=-1)\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "n = do_grid_search(X, y)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = LogisticRegression()\n",
    "fit_model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for feature importances across all trees\n",
    "\n",
    "n = len(X.columns)\n",
    "\n",
    "#importances = forest_fit.feature_importances_[:n]\n",
    "importances = model.feature_importances_[:n]\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features = list(X.columns[indices])\n",
    "\n",
    "# print(\"Feature ranking:\")\n",
    "# for f in range(n):\n",
    "#     print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(n), importances[indices], yerr=std[indices], color=\"r\", align=\"edge\", width = -.9)\n",
    "plt.xticks(range(n), features, rotation=-75)\n",
    "plt.xlim([-1, n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try modifying the number of trees, graph results\n",
    "num_trees = range(5, 50, 5)\n",
    "accuracies = []\n",
    "for n in num_trees:\n",
    "    tot = 0\n",
    "    for i in range(5):\n",
    "        rf = RandomForestClassifier(n_estimators=n)\n",
    "        rf.fit(X_train, y_train)\n",
    "        tot += rf.score(X_test, y_test)\n",
    "    accuracies.append(tot / 5)\n",
    "plt.plot(num_trees, accuracies)\n",
    "plt.xlabel=\"num_trees\"\n",
    "plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modifying the max features parameter\n",
    "for nn in range(10):\n",
    "    num_features = range(2, len(X.columns))\n",
    "    accuracies = []\n",
    "    for n in num_features:\n",
    "        tot = 0\n",
    "        for i in range(5):\n",
    "            rf = RandomForestClassifier(max_features=n)\n",
    "            rf.fit(X_train, y_train)\n",
    "            tot += rf.score(X_test, y_test)\n",
    "        accuracies.append(tot / 5)\n",
    "    plt.plot(num_features, accuracies)\n",
    "    plt.xlabel=\"num_features\"\n",
    "    plt.ylabel=\"accuracy\"\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run all the other classifiers that we have learned so far in class\n",
    "def get_scores(classifier, X_train, X_test, y_train, y_test, **kwargs):\n",
    "    model = classifier(**kwargs)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_predict = model.predict(X_test)\n",
    "    return model.score(X_test, y_test), \\\n",
    "           precision_score(y_test, y_predict), \\\n",
    "           recall_score(y_test, y_predict)\n",
    "\n",
    "print (\"    Model,                Accuracy, Precision, Recall\")\n",
    "print (\"    Random Forest:       \", get_scores(RandomForestClassifier, X_train, X_test, y_train, y_test, n_estimators=25, max_features=5))\n",
    "print (\"    Logistic Regression: \", get_scores(LogisticRegression, X_train, X_test, y_train, y_test))\n",
    "print (\"    Decision Tree:       \", get_scores(DecisionTreeClassifier, X_train, X_test, y_train, y_test))\n",
    "print (\"    Naive Bayes:         \", get_scores(MultinomialNB, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_roc(X, y, clf_class, title, **kwargs):\n",
    "# def plot_roc(X, y, clf_class, kwargs):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    kf = KFold(len(y), n_folds=5, shuffle=True)\n",
    "    y_prob = np.zeros((len(y),2))\n",
    "    mean_tpr = 0.0\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    all_tpr = []\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train = y[train_index]\n",
    "        clf = clf_class(**kwargs)\n",
    "        clf.fit(X_train,y_train)\n",
    "        # Predict probabilities, not classes\n",
    "        y_prob[test_index] = clf.predict_proba(X_test)\n",
    "        fpr, tpr, thresholds = roc_curve(y[test_index], y_prob[test_index, 1])\n",
    "        mean_tpr += interp(mean_fpr, fpr, tpr)\n",
    "        mean_tpr[0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, lw=1, label='ROC fold %d (area = %0.2f)' % (i, roc_auc))\n",
    "    mean_tpr /= len(kf)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    plt.plot(mean_fpr, mean_tpr, 'k--',label='Mean ROC (area = %0.2f)' % mean_auc, lw=2)\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], '--', color=(0.6, 0.6, 0.6), label='Random')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "    plt.title(title + 'ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xfake, yfake = np.random.rand(100,6), np.random.randint(0,2, size=(100,))\n",
    "plt.figure(figsize=(20,16))\n",
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(xfake, yfake, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "#plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "#plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Visualize the roc curve of each model\")\n",
    "plot_roc(X, y, RandomForestClassifier, 'Random_Forest', n_estimators=25, max_features=5)\n",
    "plot_roc(X, y, LogisticRegression, 'Logistic_Regrssion')\n",
    "plot_roc(X, y, DecisionTreeClassifier, 'Decision_Tree')\n",
    "#plot_roc(X, y, MultinomialNB, 'Naive_Bayes') error\n",
    "print('\\nPlotting completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
