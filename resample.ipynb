{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNPICKLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle_it(name):\n",
    "    filepath = '/Users/gandalf/Documents/data/'+name+'.pkl'\n",
    "    return pd.read_pickle(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = unpickle_it('user_df')\n",
    "# unpickle_it(message_df,'message_df')\n",
    "convo_df = unpickle_it('convo_df')\n",
    "# unpickle_it(lastmessage_df,'lastmessage_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Make sure we're not useing ECT data\")\n",
    "print(pd.to_datetime(convo_df.timestamp.max()*1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Messages with responses:    {}\".format((convo_df.response == True).sum()))\n",
    "print(\"Messages without responses: {}\".format((convo_df.response == False).sum()))\n",
    "print(\"Total messages:             {}\".format(len(convo_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET X AND y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cols = ['response','first_uid','second_uid']\n",
    "simple_df = convo_df[simple_cols]\n",
    "\n",
    "ad,sg,sr,sc,sn,ss,t = [],[],[],[],[],[],[]\n",
    "c=0\n",
    "for index, row in simple_df.iterrows():\n",
    "    try:\n",
    "        first = user_df.loc[str(row.first_uid)]\n",
    "        second = user_df.loc[str(row.second_uid)]\n",
    "        ad.append(abs(first.age - second.age))\n",
    "        sg.append(first.gender == second.gender)\n",
    "        sr.append(first.inRelationship == second.inRelationship)\n",
    "        sc.append(first.isClean == second.isClean)\n",
    "        sn.append(first.isNight == second.isNight)\n",
    "        ss.append(first.isStudent == second.isStudent)\n",
    "        t.append(False)\n",
    "    except:\n",
    "        ad.append('trouble')\n",
    "        sg.append('trouble')\n",
    "        sr.append('trouble')\n",
    "        sc.append('trouble')\n",
    "        sn.append('trouble')\n",
    "        ss.append('trouble')\n",
    "        t.append(True)\n",
    "        \n",
    "        c += 1\n",
    "\n",
    "simple_df['age_dif'] = ad\n",
    "simple_df['same_gender'] = sg\n",
    "simple_df['same_relate'] = sr\n",
    "simple_df['same_clean'] = sc\n",
    "simple_df['same_night'] = sn\n",
    "simple_df['same_student'] = ss\n",
    "simple_df['trouble'] = t\n",
    "\n",
    "simple_df = simple_df.drop(simple_df[simple_df.trouble].index)\n",
    "simple_df = simple_df.drop(['trouble'], axis=1)\n",
    "print(\"Num bad rows: {}\".format(c))\n",
    "simple_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = simple_df\n",
    "y = X.response.map({True:1,False:0})\n",
    "X = X.drop(['first_uid','second_uid','response'], axis=1)\n",
    "print(len(X))\n",
    "print(len(y))#.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)\n",
    "\n",
    "print(\"messages sent: {}\".format(len(y_train)))\n",
    "print(\"responses:     {}\\n\".format(y_train.sum()))\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Prediction:   {}\".format(model.predict(X_test)))\n",
    "print(\"Actual:       {}\".format(y_test.values))\n",
    "print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Score of model: {}\".format(model.score(X_test, y_test)))\n",
    "print(\"Score to beat: {}\".format(1-y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNDERSAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def div_count_pos_neg(X, y):\n",
    "    \"\"\"Helper function to divide X & y into positive and negative classes\n",
    "    and counts up the number in each.\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray - 2D\n",
    "    y : ndarray - 1D\n",
    "    Returns\n",
    "    -------\n",
    "    negative_count : Int\n",
    "    positive_count : Int\n",
    "    X_positives    : ndarray - 2D\n",
    "    X_negatives    : ndarray - 2D\n",
    "    y_positives    : ndarray - 1D\n",
    "    y_negatives    : ndarray - 1D\n",
    "    \"\"\"\n",
    "    negatives, positives = y == 0, y == 1\n",
    "    negative_count, positive_count = np.sum(negatives), np.sum(positives)\n",
    "    X_positives, y_positives = X[positives], y[positives]\n",
    "    X_negatives, y_negatives = X[negatives], y[negatives]\n",
    "    return negative_count, positive_count, X_positives, \\\n",
    "           X_negatives, y_positives, y_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undersample(X, y, tp):\n",
    "    \"\"\"Randomly discards negative observations from X & y to achieve the\n",
    "    target proportion of positive to negative observations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X  : ndarray - 2D\n",
    "    y  : ndarray - 1D\n",
    "    tp : float - range [0, 1], target proportion of positive class observations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_undersampled : ndarray - 2D\n",
    "    y_undersampled : ndarray - 1D\n",
    "    \"\"\"\n",
    "    if tp < np.mean(y):\n",
    "        return X, y\n",
    "    neg_count, pos_count, X_pos, X_neg, y_pos, y_neg = div_count_pos_neg(X, y)\n",
    "    negative_sample_rate = (pos_count * (1 - tp)) / (neg_count * tp)\n",
    "    negative_keepers = np.random.choice(a=[False, True], size=neg_count,\n",
    "                                        p=[1 - negative_sample_rate,\n",
    "                                           negative_sample_rate])\n",
    "    X_negative_undersampled = X_neg[negative_keepers]\n",
    "    y_negative_undersampled = y_neg[negative_keepers]\n",
    "    X_undersampled = np.vstack((X_negative_undersampled, X_pos))\n",
    "    y_undersampled = np.concatenate((y_negative_undersampled, y_pos))\n",
    "\n",
    "    return X_undersampled, y_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_und, y_und = undersample(X, y, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_und, y_und, random_state=17)\n",
    "\n",
    "print(\"messages sent: {}\".format(len(y_train)))\n",
    "print(\"responses:     {}\\n\".format(y_train.sum()))\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Prediction:   {}\".format(model.predict(X_test)))\n",
    "print(\"Actual:       {}\".format(y_test))\n",
    "print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Score of model: {}\".format(model.score(X_test, y_test)))\n",
    "print(\"Score to beat: {}\".format(1-y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OVER SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oversample(X, y, tp):\n",
    "    \"\"\"Randomly choose positive observations from X & y, with replacement\n",
    "    to achieve the target proportion of positive to negative observations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X  : ndarray - 2D\n",
    "    y  : ndarray - 1D\n",
    "    tp : float - range [0, 1], target proportion of positive class observations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_undersampled : ndarray - 2D\n",
    "    y_undersampled : ndarray - 1D\n",
    "    \"\"\"\n",
    "    if tp < np.mean(y):\n",
    "        return X, y\n",
    "    neg_count, pos_count, X_pos, X_neg, y_pos, y_neg = div_count_pos_neg(X, y)\n",
    "    positive_range = np.arange(pos_count)\n",
    "    positive_size = (tp * neg_count) / (1 - tp)\n",
    "    positive_idxs = np.random.choice(a=positive_range,\n",
    "                                     size=int(positive_size),\n",
    "                                     replace=True)\n",
    "    X_positive_oversampled = X_pos[positive_idxs]\n",
    "    y_positive_oversampled = y_pos[positive_idxs]\n",
    "    X_oversampled = np.vstack((X_positive_oversampled, X_neg))\n",
    "    y_oversampled = np.concatenate((y_positive_oversampled, y_neg))\n",
    "\n",
    "    return X_oversampled, y_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ove, y_ove = undersample(X, y, .5)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ove, y_ove, random_state=17)\n",
    "\n",
    "print(\"messages sent: {}\".format(len(y_train)))\n",
    "print(\"responses:     {}\\n\".format(y_train.sum()))\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Prediction:   {}\".format(model.predict(X_test)))\n",
    "print(\"Actual:       {}\".format(y_test))\n",
    "print(\"Model recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "print(\"Model precision: {}\".format(precision_score(y_test, y_pred)))\n",
    "print(\"Score of model: {}\".format(model.score(X_test, y_test)))\n",
    "print(\"Score to beat: {}\".format(1-y_test.sum()/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
