{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import my_pickle as mp\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mp.unjson_it('data_master')\n",
    "df = df.fillna({\"about_sender\":'',\"about_receiver\":''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "about_sender = list(df.about_sender.values)\n",
    "about_receiver = list(df.about_receiver.values)\n",
    "all_text = about_sender+about_receiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,blurb in enumerate(all_text):\n",
    "#     try: all_text[i] = blurb.lower()\n",
    "#     except: all_text[i] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized = [word_tokenize(content) for content in sender_blurbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop = set(stopwords.words('english'))\n",
    "# docs = [[word for word in words if word not in stop]\n",
    "#         for words in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer('english')\n",
    "# wordnet = WordNetLemmatizer()\n",
    "\n",
    "# docs_porter = [[porter.stem(word) for word in words]\n",
    "#                for words in docs]\n",
    "# docs_snowball = [[snowball.stem(word) for word in words]\n",
    "#                  for words in docs]\n",
    "# docs_wordnet = [[wordnet.lemmatize(word) for word in words]\n",
    "#                 for words in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Print the stemmed and lemmatized words from the nth document\n",
    "# nth = 9\n",
    "# print (\"%16s %16s %16s %16s\" % (\"word\", \"porter\", \"snowball\", \"lemmatizer\"))\n",
    "# for i in range(min(len(docs_porter[nth]), len(docs_snowball[nth]), len(docs_wordnet[nth]))):\n",
    "#     p, s, w = docs_porter[nth][i], docs_snowball[nth][i], docs_wordnet[nth][i]\n",
    "#     if len(set((p, s, w))) != 1:\n",
    "#         print (\"%16s %16s %16s %16s\" % (docs[nth][i], p, s, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer(max_df=1, \n",
    "#                              min_df=0, \n",
    "#                              stop_words='english', \n",
    "#                              max_features = 10000,\n",
    "#                              ngram_range =(1,2),\n",
    "#                              norm = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_docs = docs_snowball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    '''\n",
    "    INPUT: string\n",
    "    OUTPUT: list of strings\n",
    "\n",
    "    Tokenize and stem/lemmatize the document.\n",
    "    '''\n",
    "    snowball = SnowballStemmer('english')\n",
    "    return [snowball.stem(word) for word in word_tokenize(doc.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "count_vectorized = countvect.fit(all_text)\n",
    "count_sender = countvect.transform(about_sender)\n",
    "count_receiver = countvect.transform(about_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer(stop_words='english', tokenizer=tokenize)\n",
    "tfidf_vectorized = tfidfvect.fit(all_text)\n",
    "tfidf_sender = tfidfvect.transform(about_sender)\n",
    "tfidf_receiver = tfidfvect.transform(about_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_cosine_similarities = linear_kernel(tfidf_sender, tfidf_receiver)\n",
    "count_cosine_similarities = linear_kernel(count_sender, count_receiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_similarity = []\n",
    "count_similarity = []\n",
    "for i in range(len(df)):\n",
    "    tfidf_similarity.append(tfidf_cosine_similarities[i][i])\n",
    "    count_similarity.append(count_cosine_similarities[i][i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tfidf_similarity'] = tfidf_similarity\n",
    "df['count_similarity'] = count_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_similarity_df = df[['tfidf_similarity','count_similarity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mp.json_it(text_similarity_df,'data_text_similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# words = countvect.get_feature_names()\n",
    "# print( \"sklearn count of 'travel':\", count_vectorized[0, words.index('travel')])\n",
    "# # print( \"my count of 'dinner':\", word_counts[0, vocab_dict['travel']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tfidf_vectorized = tfidfvect.fit_transform(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_sender.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarities[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = []\n",
    "# for i in range(len(df)):\n",
    "    sim.append(cosine_similarities[i][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figsize = (16,8)\n",
    "plt.scatter(df.convo_length, df.text_similarity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.response==True].text_similarity.mean()\n",
    "df[df.response==False].text_similarity.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities[7,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_blurbs[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_blurbs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender_blurbs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
